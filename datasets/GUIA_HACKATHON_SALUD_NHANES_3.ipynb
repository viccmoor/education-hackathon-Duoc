{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè• Gu√≠a Hackathon Salud NHANES - Coach de Bienestar Preventivo\n",
    "## Duoc UC 2025 - 27 horas de desarrollo\n",
    "\n",
    "**Objetivo:** Sistema h√≠brido ML + LLM para predicci√≥n de riesgo cardiometab√≥lico y coaching personalizado\n",
    "\n",
    "### üìã Checklist de Entregables\n",
    "- [ ] Modelo ML con AUROC ‚â• 0.80\n",
    "- [ ] API FastAPI con /predict y /coach\n",
    "- [ ] App en Streamlit/Gradio deployada en HF Spaces\n",
    "- [ ] Validaci√≥n temporal sin fuga de datos\n",
    "- [ ] M√©tricas de fairness por subgrupos\n",
    "- [ ] RAG con citas a /kb local\n",
    "- [ ] PDF descargable del plan\n",
    "- [ ] Presentaci√≥n de 10 min\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ FASE 0: Setup Inicial (30 min - H0 a H0.5)\n",
    "\n",
    "### Instalaci√≥n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias desde requirements.txt\n",
    "# Esto asegura que todas las librer√≠as est√©n en las versiones correctas\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Si prefieres instalar individualmente (no recomendado):\n",
    "# !pip install -q pandas numpy scikit-learn xgboost lightgbm \\\n",
    "#     fastapi uvicorn pydantic streamlit gradio \\\n",
    "#     openai shap matplotlib seaborn plotly \\\n",
    "#     reportlab fpdf rank-bm25 python-multipart joblib requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup completado\n"
     ]
    }
   ],
   "source": [
    "# Imports generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Estructura de Datos Esperada\n",
    "\n",
    "**IMPORTANTE:** Antes de ejecutar el c√≥digo, aseg√∫rate de tener los datos NHANES en formato CSV.\n",
    "\n",
    "**Estructura de directorios esperada:**\n",
    "```\n",
    "./data/\n",
    "‚îú‚îÄ‚îÄ DEMO_2007_2008.csv    # Demographics (OBLIGATORIO)\n",
    "‚îú‚îÄ‚îÄ EXAM_2007_2008.csv    # Examination (recomendado)\n",
    "‚îú‚îÄ‚îÄ LAB_2007_2008.csv     # Laboratory (recomendado para labels)\n",
    "‚îú‚îÄ‚îÄ QUEST_2007_2008.csv   # Questionnaire (opcional)\n",
    "‚îú‚îÄ‚îÄ DIET_2007_2008.csv    # Dietary (opcional)\n",
    "‚îú‚îÄ‚îÄ DEMO_2009_2010.csv\n",
    "‚îú‚îÄ‚îÄ EXAM_2009_2010.csv\n",
    "‚îú‚îÄ‚îÄ LAB_2009_2010.csv\n",
    "‚îî‚îÄ‚îÄ ... (y as√≠ para cada ciclo)\n",
    "```\n",
    "\n",
    "**Formato de nombres:**\n",
    "- `DEMO_{CICLO}.csv` donde CICLO = `2007_2008` (con gui√≥n bajo)\n",
    "- `EXAM_{CICLO}.csv`\n",
    "- `LAB_{CICLO}.csv`\n",
    "- `QUEST_{CICLO}.csv`\n",
    "- `DIET_{CICLO}.csv`\n",
    "\n",
    "**Ciclos soportados:**\n",
    "- Entrenamiento: `2007-2008`, `2009-2010`, `2011-2012`, `2013-2014`, `2015-2016`\n",
    "- Test: `2017-2018`\n",
    "\n",
    "**Columna obligatoria:**\n",
    "- Todos los archivos deben tener la columna `SEQN` (ID √∫nico del participante)\n",
    "\n",
    "Si no tienes los datos, el c√≥digo mostrar√° mensajes de advertencia pero no fallar√°.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è IMPORTANTE: Los Datos NHANES NO Vienen en CSV\n",
    "\n",
    "**Los datos de NHANES se distribuyen en formato SAS Transport File (.XPT) y necesitan ser convertidos a CSV.**\n",
    "\n",
    "**Pasos para obtener los datos:**\n",
    "\n",
    "1. **Descargar archivos .XPT:**\n",
    "\n",
    "   **Opci√≥n A - Descarga Autom√°tica (Intentar primero):**\n",
    "   ```bash\n",
    "   # Descargar un m√≥dulo de prueba\n",
    "   python descargar_nhanes.py --cycle 2017-2018 --module DEMO\n",
    "   \n",
    "   # Descargar m√∫ltiples m√≥dulos\n",
    "   python descargar_nhanes.py --cycle 2017-2018 --module DEMO EXAM LAB\n",
    "   ```\n",
    "   \n",
    "   ‚ö†Ô∏è Si la descarga autom√°tica falla (com√∫n por protecciones del sitio), el script te dar√° instrucciones claras para descarga manual.\n",
    "   \n",
    "   **Opci√≥n B - Descarga Manual:**\n",
    "   - Sitio oficial: https://wwwn.cdc.gov/nchs/nhanes/Default.aspx\n",
    "   - Selecciona el ciclo (ej: 2007-2008)\n",
    "   - Descarga los m√≥dulos: DEMO, EXAM, LAB, QUEST\n",
    "   - Coloca los archivos .XPT en `./data/`\n",
    "\n",
    "2. **Convertir .XPT a CSV:**\n",
    "\n",
    "   **Opci√≥n A - Script Simple (Recomendado):**\n",
    "   ```bash\n",
    "   python convertir_nhanes.py\n",
    "   ```\n",
    "   \n",
    "   **Opci√≥n B - Script Completo:**\n",
    "   ```python\n",
    "   from nhanes_data_converter import convert_xpt_to_csv\n",
    "   from pathlib import Path\n",
    "   \n",
    "   # Convertir todos los .XPT en ./data/\n",
    "   for xpt_file in Path('./data').glob('*.XPT'):\n",
    "       convert_xpt_to_csv(xpt_file)\n",
    "   ```\n",
    "   \n",
    "   **Opci√≥n C - Manualmente:**\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   df = pd.read_sas('DEMO_2007_2008.XPT', encoding='utf-8')\n",
    "   df.to_csv('DEMO_2007_2008.csv', index=False)\n",
    "   ```\n",
    "\n",
    "3. **Colocar archivos CSV en `./data/` con el formato:**\n",
    "   - `DEMO_2007_2008.csv` (o `DEMO_J.csv` para ciclo 2017-2018)\n",
    "   - `EXAM_2007_2008.csv`\n",
    "   - `LAB_2007_2008.csv`\n",
    "   - `QUEST_2007_2008.csv`\n",
    "\n",
    "**üìñ Ver gu√≠as completas:**\n",
    "- `README.md` - Documentaci√≥n completa\n",
    "- `QUICK_START.md` - Gu√≠a de inicio r√°pido\n",
    "- `CONVERSION_DATOS_NHANES.md` - Gu√≠a detallada de conversi√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä FASE 1: MANEJO DE DATOS NHANES (H0.5 a H4) \n",
    "### ‚ö†Ô∏è LA PARTE M√ÅS CR√çTICA DEL HACKATHON\n",
    "\n",
    "### 1.1 Entender la estructura NHANES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìñ CONCEPTOS CLAVE DE NHANES:\n",
    "\n",
    "**Estructura de datos:**\n",
    "- **SEQN**: ID √∫nico de participante (clave para hacer merge)\n",
    "- **Ciclos**: Per√≠odos de 2 a√±os (2007-2008, 2009-2010, etc.)\n",
    "- **M√≥dulos separados**: Demographics, Examination, Laboratory, Questionnaire, Dietary\n",
    "\n",
    "**Pesos muestrales (WEIGHTS):**\n",
    "- `WTMEC2YR`: Peso para ex√°menes m√©dicos (2 a√±os)\n",
    "- `WTINT2YR`: Peso para entrevistas\n",
    "- `WTDRD1`: Peso para datos diet√©ticos\n",
    "- **Regla:** Usar el peso m√°s restrictivo del an√°lisis\n",
    "\n",
    "**Dise√±o complejo:**\n",
    "- `SDMVPSU`: Unidad primaria de muestreo (cluster)\n",
    "- `SDMVSTRA`: Estrato de muestreo\n",
    "- Necesario para estad√≠sticas poblacionales correctas\n",
    "\n",
    "**Variables t√≠picas:**\n",
    "- Demographics: `RIDAGEYR` (edad), `RIAGENDR` (sexo), `RIDRETH3` (etnia)\n",
    "- Examination: `BMXWT` (peso), `BMXHT` (altura), `BMXWAIST` (cintura)\n",
    "- BP: `BPXSY1`, `BPXDI1` (presi√≥n arterial)\n",
    "- Lab: `LBXGH` (A1c), `LBXGLU` (glucosa)\n",
    "- Questionnaire: `SLQ050` (sue√±o), `SMQ020` (fumador), `PAQ605` (actividad f√≠sica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Funci√≥n de carga robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nhanes_data(data_dir='/home/diego/Documentos/GitHub/education-hackathon-Duoc/machin/datasets', cycles=None):\n",
    "    \"\"\"\n",
    "    Carga y merge de datos NHANES por ciclo.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directorio con archivos CSV\n",
    "        cycles: Lista de ciclos a cargar, ej: ['2007-2008', '2009-2010']\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame consolidado\n",
    "        metadata: Diccionario con informaci√≥n del merge\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Verificar que el directorio existe\n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ö†Ô∏è ERROR: El directorio {data_path} no existe\")\n",
    "        print(f\"   Crea el directorio y coloca los archivos CSV all√≠\")\n",
    "        print(f\"   Estructura esperada:\")\n",
    "        print(f\"   {data_path\n",
    "                    \n",
    "                    }/\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ DEMO_2007_2008.csv\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ EXAM_2007_2008.csv\")\n",
    "        print(f\"     ‚îú‚îÄ‚îÄ LAB_2007_2008.csv\")\n",
    "        print(f\"     ‚îî‚îÄ‚îÄ QUEST_2007_2008.csv\")\n",
    "        return pd.DataFrame(), {'cycles': [], 'modules': [], 'n_participants': {}}\n",
    "    \n",
    "    if cycles is None:\n",
    "        # Ciclos de entrenamiento\n",
    "        cycles = ['2007-2008', '2009-2010', '2011-2012', '2013-2014', '2015-2016']\n",
    "    \n",
    "    all_data = []\n",
    "    metadata = {'cycles': cycles, 'modules': [], 'n_participants': {}}\n",
    "    \n",
    "    for cycle in cycles:\n",
    "        print(f\"\\nüìÅ Cargando ciclo {cycle}...\")\n",
    "        cycle_data = None\n",
    "        \n",
    "        # 1. DEMOGRAPHICS (siempre la base)\n",
    "        # Intentar ambos formatos: DEMO_2017_2018.csv (con gui√≥n bajo) y DEMO_J.csv (con letra)\n",
    "        letter = CYCLE_TO_LETTER.get(cycle, '')\n",
    "        demo_file = data_path / f\"DEMO_{cycle.replace('-', '_')}.csv\"\n",
    "        if not demo_file.exists() and letter:\n",
    "            # Intentar formato con letra (ej: DEMO_J.csv)\n",
    "            demo_file = data_path / f\"DEMO_{letter}.csv\"\n",
    "        if demo_file.exists():\n",
    "            try:\n",
    "                demo = pd.read_csv(demo_file)\n",
    "                if 'SEQN' not in demo.columns:\n",
    "                    print(f\"  ‚ö†Ô∏è ERROR: Columna SEQN no encontrada en {demo_file}\")\n",
    "                    print(f\"     Este archivo no es v√°lido para merge\")\n",
    "                    continue\n",
    "                demo['CYCLE'] = cycle\n",
    "                cycle_data = demo\n",
    "                print(f\"  ‚úì Demographics: {len(demo):,} registros, {len(demo.columns)} columnas\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {demo_file}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Demographics no encontrado: {demo_file}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. EXAMINATION (antropometr√≠a y PA)\n",
    "        # Intentar ambos formatos para EXAM\n",
    "        exam_file = data_path / f\"EXAM_{cycle.replace('-', '_')}.csv\"\n",
    "        if not exam_file.exists() and letter:\n",
    "            exam_file = data_path / f\"EXAM_{letter}.csv\"\n",
    "        if exam_file.exists():\n",
    "            try:\n",
    "                exam = pd.read_csv(exam_file)\n",
    "                if 'SEQN' in exam.columns:\n",
    "                    cycle_data = cycle_data.merge(exam, on='SEQN', how='left', suffixes=('', '_exam'))\n",
    "                    print(f\"  ‚úì Examination: {len(exam):,} registros merged, {len(exam.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {exam_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {exam_file}: {e}\")\n",
    "        \n",
    "        # 3. LABORATORY (A1c, glucosa - SOLO PARA LABEL)\n",
    "        # Intentar ambos formatos para LAB\n",
    "        lab_file = data_path / f\"LAB_{cycle.replace('-', '_')}.csv\"\n",
    "        if not lab_file.exists() and letter:\n",
    "            lab_file = data_path / f\"LAB_{letter}.csv\"\n",
    "        if lab_file.exists():\n",
    "            try:\n",
    "                lab = pd.read_csv(lab_file)\n",
    "                if 'SEQN' in lab.columns:\n",
    "                    # CR√çTICO: Marcar columnas de lab para no usarlas como features\n",
    "                    lab_cols = [c for c in lab.columns if c != 'SEQN']\n",
    "                    lab = lab.rename(columns={c: f'LAB_{c}' for c in lab_cols})\n",
    "                    cycle_data = cycle_data.merge(lab, on='SEQN', how='left')\n",
    "                    print(f\"  ‚úì Laboratory: {len(lab):,} registros merged (SOLO PARA LABEL), {len(lab_cols)} columnas de lab\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {lab_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {lab_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Laboratory no encontrado: {lab_file} (opcional pero recomendado)\")\n",
    "        \n",
    "        # 4. QUESTIONNAIRE (sue√±o, actividad, tabaco)\n",
    "        # Intentar ambos formatos para QUEST\n",
    "        quest_file = data_path / f\"QUEST_{cycle.replace('-', '_')}.csv\"\n",
    "        if not quest_file.exists() and letter:\n",
    "            quest_file = data_path / f\"QUEST_{letter}.csv\"\n",
    "        if quest_file.exists():\n",
    "            try:\n",
    "                quest = pd.read_csv(quest_file)\n",
    "                if 'SEQN' in quest.columns:\n",
    "                    cycle_data = cycle_data.merge(quest, on='SEQN', how='left', suffixes=('', '_quest'))\n",
    "                    print(f\"  ‚úì Questionnaire: {len(quest):,} registros merged, {len(quest.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {quest_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {quest_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Questionnaire no encontrado: {quest_file} (opcional)\")\n",
    "        \n",
    "        # 5. DIETARY (opcional, nutrici√≥n)\n",
    "        # Intentar ambos formatos para DIET\n",
    "        diet_file = data_path / f\"DIET_{cycle.replace('-', '_')}.csv\"\n",
    "        if not diet_file.exists() and letter:\n",
    "            diet_file = data_path / f\"DIET_{letter}.csv\"\n",
    "        if diet_file.exists():\n",
    "            try:\n",
    "                diet = pd.read_csv(diet_file)\n",
    "                if 'SEQN' in diet.columns:\n",
    "                    cycle_data = cycle_data.merge(diet, on='SEQN', how='left', suffixes=('', '_diet'))\n",
    "                    print(f\"  ‚úì Dietary: {len(diet):,} registros merged, {len(diet.columns)} columnas\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Columna SEQN no encontrada en {diet_file}, saltando merge\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è ERROR leyendo {diet_file}: {e}\")\n",
    "        \n",
    "        if cycle_data is not None:\n",
    "            all_data.append(cycle_data)\n",
    "            metadata['n_participants'][cycle] = len(cycle_data)\n",
    "    \n",
    "    # Concatenar todos los ciclos\n",
    "    if all_data:\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\n‚úÖ TOTAL: {len(df):,} participantes cargados\")\n",
    "        print(f\"   Columnas totales: {df.shape[1]}\")\n",
    "        print(f\"   Ciclos cargados: {len(all_data)}/{len(cycles)}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è ADVERTENCIA: No se cargaron datos de ning√∫n ciclo\")\n",
    "        print(f\"   Verifica que los archivos CSV est√©n en {data_path}\")\n",
    "        df = pd.DataFrame()\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "# CARGAR DATOS DE ENTRENAMIENTO\n",
    "df_train, meta = load_nhanes_data(cycles=['2007-2008', '2009-2010', '2011-2012', '2013-2014', '2015-2016'])\n",
    "\n",
    "# CARGAR DATOS DE TEST (ciclo ciego)\n",
    "df_test, meta_test = load_nhanes_data(cycles=['2017-2018'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploraci√≥n inicial cr√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar distribuci√≥n por ciclo\n",
    "print(\"üìä Distribuci√≥n por ciclo:\")\n",
    "print(df_train['CYCLE'].value_counts().sort_index())\n",
    "print(f\"\\nüìä Test set: {df_test['CYCLE'].value_counts()}\")\n",
    "\n",
    "# Variables clave disponibles\n",
    "print(\"\\nüîë Variables clave disponibles:\")\n",
    "key_vars = {\n",
    "    'Demographics': ['RIDAGEYR', 'RIAGENDR', 'RIDRETH3'],\n",
    "    'Anthropometry': ['BMXWT', 'BMXHT', 'BMXWAIST', 'BMXBMI'],\n",
    "    'Blood Pressure': ['BPXSY1', 'BPXSY2', 'BPXDI1', 'BPXDI2'],\n",
    "    'Laboratory': ['LAB_LBXGH', 'LAB_LBXGLU'],  # Prefijo LAB_\n",
    "    'Sleep': ['SLD010H', 'SLD012'],\n",
    "    'Smoking': ['SMQ020', 'SMD030'],\n",
    "    'Physical Activity': ['PAQ605', 'PAQ620', 'PAD680']\n",
    "}\n",
    "\n",
    "for module, vars in key_vars.items():\n",
    "    available = [v for v in vars if v in df_train.columns]\n",
    "    print(f\"  {module}: {len(available)}/{len(vars)} disponibles\")\n",
    "    if len(available) < len(vars):\n",
    "        missing = [v for v in vars if v not in df_train.columns]\n",
    "        print(f\"    ‚ö†Ô∏è Faltantes: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limpieza y tipado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nhanes_data(df):\n",
    "    \"\"\"\n",
    "    Limpieza est√°ndar de datos NHANES.\n",
    "    \n",
    "    Maneja:\n",
    "    - Valores missing codificados (77777, 99999, etc.)\n",
    "    - Conversi√≥n de tipos\n",
    "    - Rangos v√°lidos\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Valores missing codificados en NHANES\n",
    "    missing_codes = [7, 9, 77, 99, 777, 999, 7777, 9999, 77777, 99999, '.', '']\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col] = df[col].replace(missing_codes, np.nan)\n",
    "    \n",
    "    # 2. Edad: solo adultos\n",
    "    if 'RIDAGEYR' in df.columns:\n",
    "        df = df[df['RIDAGEYR'] >= 18].copy()\n",
    "        df = df[df['RIDAGEYR'] <= 85].copy()  # L√≠mite superior\n",
    "    \n",
    "    # 3. Sexo: 1=M, 2=F\n",
    "    if 'RIAGENDR' in df.columns:\n",
    "        df['sex'] = df['RIAGENDR'].map({1: 'M', 2: 'F'})\n",
    "    \n",
    "    # 4. Antropometr√≠a: rangos razonables\n",
    "    if 'BMXWT' in df.columns:  # Peso en kg\n",
    "        df.loc[(df['BMXWT'] < 30) | (df['BMXWT'] > 250), 'BMXWT'] = np.nan\n",
    "    \n",
    "    if 'BMXHT' in df.columns:  # Altura en cm\n",
    "        df.loc[(df['BMXHT'] < 120) | (df['BMXHT'] > 220), 'BMXHT'] = np.nan\n",
    "    \n",
    "    if 'BMXWAIST' in df.columns:  # Cintura en cm\n",
    "        df.loc[(df['BMXWAIST'] < 40) | (df['BMXWAIST'] > 200), 'BMXWAIST'] = np.nan\n",
    "    \n",
    "    # 5. Presi√≥n arterial: rangos v√°lidos\n",
    "    for bp_col in ['BPXSY1', 'BPXSY2', 'BPXSY3']:\n",
    "        if bp_col in df.columns:\n",
    "            df.loc[(df[bp_col] < 70) | (df[bp_col] > 250), bp_col] = np.nan\n",
    "    \n",
    "    for bp_col in ['BPXDI1', 'BPXDI2', 'BPXDI3']:\n",
    "        if bp_col in df.columns:\n",
    "            df.loc[(df[bp_col] < 30) | (df[bp_col] > 150), bp_col] = np.nan\n",
    "    \n",
    "    print(f\"‚úÖ Limpieza completada: {len(df):,} registros v√°lidos\")\n",
    "    return df\n",
    "\n",
    "# Aplicar limpieza\n",
    "df_train = clean_nhanes_data(df_train)\n",
    "df_test = clean_nhanes_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 ‚ö†Ô∏è ANTI-FUGA: Identificar columnas de laboratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CR√çTICO: Identificar todas las columnas de laboratorio\n",
    "LAB_COLUMNS = [col for col in df_train.columns if col.startswith('LAB_')]\n",
    "\n",
    "print(f\"üö® COLUMNAS DE LABORATORIO IDENTIFICADAS (NO USAR COMO FEATURES):\")\n",
    "print(f\"   Total: {len(LAB_COLUMNS)} columnas\")\n",
    "for col in LAB_COLUMNS[:20]:  # Mostrar primeras 20\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# Guardar lista para referencia\n",
    "with open('LAB_COLUMNS_FORBIDDEN.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(LAB_COLUMNS))\n",
    "\n",
    "print(\"\\n‚úÖ Lista guardada en LAB_COLUMNS_FORBIDDEN.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ FASE 2: CREACI√ìN DE LABELS (H4 a H5)\n",
    "\n",
    "### 2.1 Label A: Alto riesgo de diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diabetes_label(df):\n",
    "    \"\"\"\n",
    "    Etiqueta de alto riesgo de diabetes usando A1c y/o glucosa.\n",
    "    \n",
    "    Criterios:\n",
    "    - A1c >= 6.5% (48 mmol/mol) -> Diabetes\n",
    "    - A1c 5.7-6.4% (39-47 mmol/mol) -> Prediabetes\n",
    "    - Glucosa en ayunas >= 126 mg/dL -> Diabetes\n",
    "    - Glucosa 100-125 mg/dL -> Prediabetes\n",
    "    \n",
    "    Label = 1 si Diabetes o Prediabetes avanzada (A1c >= 6.0)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Variables de laboratorio (ajustar seg√∫n nombres reales)\n",
    "    a1c_col = 'LAB_LBXGH'  # A1c (%), ajustar nombre\n",
    "    glucose_col = 'LAB_LBXGLU'  # Glucosa (mg/dL), ajustar nombre\n",
    "    \n",
    "    # Inicializar label\n",
    "    df['label_diabetes'] = 0\n",
    "    \n",
    "    # Verificar qu√© columnas est√°n disponibles\n",
    "    has_a1c = a1c_col in df.columns\n",
    "    has_glucose = glucose_col in df.columns\n",
    "    \n",
    "    if not has_a1c and not has_glucose:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No se encontraron columnas de laboratorio (LAB_LBXGH o LAB_LBXGLU)\")\n",
    "        print(\"   Aseg√∫rate de que los archivos LAB_*.csv est√©n cargados correctamente\")\n",
    "        return df\n",
    "    \n",
    "    # Criterio 1: A1c\n",
    "    if has_a1c:\n",
    "        # Filtrar valores v√°lidos antes de comparar\n",
    "        valid_a1c = df[a1c_col].notna()\n",
    "        df.loc[valid_a1c & (df[a1c_col] >= 6.0), 'label_diabetes'] = 1\n",
    "        print(f\"  A1c >= 6.0%: {(valid_a1c & (df[a1c_col] >= 6.0)).sum():,} casos\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Columna {a1c_col} no encontrada\")\n",
    "    \n",
    "    # Criterio 2: Glucosa\n",
    "    if has_glucose:\n",
    "        # Filtrar valores v√°lidos antes de comparar\n",
    "        valid_glucose = df[glucose_col].notna()\n",
    "        df.loc[valid_glucose & (df[glucose_col] >= 110), 'label_diabetes'] = 1\n",
    "        print(f\"  Glucosa >= 110 mg/dL: {(valid_glucose & (df[glucose_col] >= 110)).sum():,} casos\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Columna {glucose_col} no encontrada\")\n",
    "    \n",
    "    # Remover casos sin datos de laboratorio\n",
    "    lab_cols = []\n",
    "    if has_a1c:\n",
    "        lab_cols.append(a1c_col)\n",
    "    if has_glucose:\n",
    "        lab_cols.append(glucose_col)\n",
    "    \n",
    "    if lab_cols:\n",
    "        has_lab_data = df[lab_cols].notna().any(axis=1)\n",
    "        df = df[has_lab_data].copy()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No hay datos de laboratorio disponibles para filtrar\")\n",
    "        return df\n",
    "    \n",
    "    prevalence = df['label_diabetes'].mean()\n",
    "    print(f\"\\n‚úÖ Label Diabetes creado:\")\n",
    "    print(f\"   Prevalencia: {prevalence:.1%}\")\n",
    "    print(f\"   n = {df['label_diabetes'].sum():,} / {len(df):,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Crear labels\n",
    "df_train = create_diabetes_label(df_train)\n",
    "df_test = create_diabetes_label(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Label B (alternativa): Hipertensi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hypertension_label(df):\n",
    "    \"\"\"\n",
    "    Etiqueta de hipertensi√≥n usando mediciones de PA.\n",
    "    \n",
    "    Criterios (Estadio 1+):\n",
    "    - Sist√≥lica >= 130 mmHg, o\n",
    "    - Diast√≥lica >= 80 mmHg\n",
    "    \n",
    "    Usa promedio de 2-3 mediciones disponibles.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calcular promedio de mediciones de PA\n",
    "    sys_cols = [c for c in df.columns if c.startswith('BPXSY') and len(c) > 5 and c[5:].isdigit()]\n",
    "    dia_cols = [c for c in df.columns if c.startswith('BPXDI') and len(c) > 5 and c[5:].isdigit()]\n",
    "    \n",
    "    if not sys_cols and not dia_cols:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: No se encontraron columnas de presi√≥n arterial (BPXSY* o BPXDI*)\")\n",
    "        print(\"   Aseg√∫rate de que los archivos EXAM_*.csv est√©n cargados correctamente\")\n",
    "        return df\n",
    "    \n",
    "    # Inicializar columnas\n",
    "    if sys_cols:\n",
    "        df['sbp_mean'] = df[sys_cols].mean(axis=1)\n",
    "        print(f\"  Columnas sist√≥lica encontradas: {sys_cols}\")\n",
    "    else:\n",
    "        df['sbp_mean'] = np.nan\n",
    "        print(\"  ‚ö†Ô∏è No se encontraron columnas de presi√≥n sist√≥lica\")\n",
    "    \n",
    "    if dia_cols:\n",
    "        df['dbp_mean'] = df[dia_cols].mean(axis=1)\n",
    "        print(f\"  Columnas diast√≥lica encontradas: {dia_cols}\")\n",
    "    else:\n",
    "        df['dbp_mean'] = np.nan\n",
    "        print(\"  ‚ö†Ô∏è No se encontraron columnas de presi√≥n diast√≥lica\")\n",
    "    \n",
    "    # Criterio de hipertensi√≥n (solo si tenemos al menos una medida)\n",
    "    df['label_hypertension'] = 0\n",
    "    has_bp_data = df[['sbp_mean', 'dbp_mean']].notna().any(axis=1)\n",
    "    \n",
    "    if has_bp_data.any():\n",
    "        # Aplicar criterio solo donde hay datos\n",
    "        mask = (df['sbp_mean'].notna() & (df['sbp_mean'] >= 130)) | \\\n",
    "               (df['dbp_mean'].notna() & (df['dbp_mean'] >= 80))\n",
    "        df.loc[mask, 'label_hypertension'] = 1\n",
    "    \n",
    "    # Remover casos sin datos de PA\n",
    "    df = df[has_bp_data].copy()\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        prevalence = df['label_hypertension'].mean()\n",
    "        print(f\"\\n‚úÖ Label Hipertensi√≥n creado:\")\n",
    "        print(f\"   Prevalencia: {prevalence:.1%}\")\n",
    "        print(f\"   n = {df['label_hypertension'].sum():,} / {len(df):,}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ADVERTENCIA: No hay datos de presi√≥n arterial disponibles\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Opcional: crear tambi√©n label de hipertensi√≥n\n",
    "df_train = create_hypertension_label(df_train)\n",
    "df_test = create_hypertension_label(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß FASE 3: INGENIER√çA DE FEATURES (H5 a H7)\n",
    "\n",
    "### 3.1 Features demogr√°ficas y antropom√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Crea features derivadas SOLO de variables permitidas.\n",
    "    \n",
    "    NO usar columnas de laboratorio (LAB_*).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. ANTROPOMETR√çA\n",
    "    # IMC (Body Mass Index)\n",
    "    if 'BMXWT' in df.columns and 'BMXHT' in df.columns:\n",
    "        df['bmi'] = df['BMXWT'] / ((df['BMXHT'] / 100) ** 2)\n",
    "        # Categor√≠as de IMC\n",
    "        df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                                     bins=[0, 18.5, 25, 30, 100],\n",
    "                                     labels=['underweight', 'normal', 'overweight', 'obese'])\n",
    "    \n",
    "    # Ratio cintura-altura (mejor predictor que IMC)\n",
    "    if 'BMXWAIST' in df.columns and 'BMXHT' in df.columns:\n",
    "        df['waist_height_ratio'] = df['BMXWAIST'] / df['BMXHT']\n",
    "        # Riesgo si >= 0.5\n",
    "        df['high_waist_height_ratio'] = (df['waist_height_ratio'] >= 0.5).astype(int)\n",
    "    \n",
    "    # 2. EDAD\n",
    "    if 'RIDAGEYR' in df.columns:\n",
    "        df['age'] = df['RIDAGEYR']\n",
    "        # Grupos etarios\n",
    "        df['age_group'] = pd.cut(df['age'], \n",
    "                                  bins=[0, 30, 45, 60, 100],\n",
    "                                  labels=['18-30', '31-45', '46-60', '60+'])\n",
    "        # Edad al cuadrado (relaci√≥n no lineal)\n",
    "        df['age_squared'] = df['age'] ** 2\n",
    "    \n",
    "    # 3. SEXO (ya mapeado en limpieza)\n",
    "    if 'sex' in df.columns:\n",
    "        df['sex_male'] = (df['sex'] == 'M').astype(int)\n",
    "    \n",
    "    # 4. SUE√ëO\n",
    "    sleep_cols = ['SLD010H', 'SLD012']  # Horas de sue√±o\n",
    "    for col in sleep_cols:\n",
    "        if col in df.columns:\n",
    "            df['sleep_hours'] = df[col]\n",
    "            # Sue√±o insuficiente (<7h) o excesivo (>9h)\n",
    "            df['poor_sleep'] = ((df['sleep_hours'] < 7) | (df['sleep_hours'] > 9)).astype(int)\n",
    "            break\n",
    "    \n",
    "    # 5. TABAQUISMO\n",
    "    if 'SMQ020' in df.columns:  # ¬øHa fumado 100+ cigarrillos?\n",
    "        df['ever_smoker'] = (df['SMQ020'] == 1).astype(int)\n",
    "    \n",
    "    if 'SMD030' in df.columns:  # Cigarrillos por d√≠a\n",
    "        df['cigarettes_per_day'] = df['SMD030']\n",
    "        df['current_smoker'] = (df['cigarettes_per_day'] > 0).astype(int)\n",
    "    \n",
    "    # 6. ACTIVIDAD F√çSICA\n",
    "    # PAQ605: d√≠as de actividad vigorosa\n",
    "    # PAQ620: d√≠as de actividad moderada\n",
    "    if 'PAQ605' in df.columns and 'PAQ620' in df.columns:\n",
    "        df['total_active_days'] = df['PAQ605'].fillna(0) + df['PAQ620'].fillna(0)\n",
    "        # Cumple recomendaciones (150+ min/semana ‚âà 5 d√≠as)\n",
    "        df['meets_activity_guidelines'] = (df['total_active_days'] >= 5).astype(int)\n",
    "    \n",
    "    # 7. INTERACCIONES IMPORTANTES\n",
    "    if 'bmi' in df.columns and 'age' in df.columns:\n",
    "        df['bmi_age_interaction'] = df['bmi'] * df['age']\n",
    "    \n",
    "    if 'waist_height_ratio' in df.columns and 'age' in df.columns:\n",
    "        df['waist_age_interaction'] = df['waist_height_ratio'] * df['age']\n",
    "    \n",
    "    print(f\"‚úÖ Features creadas: {df.shape[1]} columnas totales\")\n",
    "    \n",
    "    # Listar nuevas features num√©ricas\n",
    "    new_features = ['bmi', 'waist_height_ratio', 'age', 'age_squared',\n",
    "                    'sleep_hours', 'cigarettes_per_day', 'total_active_days',\n",
    "                    'bmi_age_interaction', 'waist_age_interaction']\n",
    "    available_features = [f for f in new_features if f in df.columns]\n",
    "    \n",
    "    print(f\"\\nüìä Features num√©ricas disponibles: {len(available_features)}\")\n",
    "    for feat in available_features:\n",
    "        if feat in df.columns:\n",
    "            missing_pct = df[feat].isna().mean() * 100\n",
    "            print(f\"   - {feat}: {missing_pct:.1f}% missing\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar ingenier√≠a de features\n",
    "df_train = engineer_features(df_train)\n",
    "df_test = engineer_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Selecci√≥n final de features (sin fuga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_no_leakage(df, target='label_diabetes'):\n",
    "    \"\"\"\n",
    "    Selecciona features finales SIN columnas de laboratorio.\n",
    "    \n",
    "    CR√çTICO: Valida que no haya fuga de datos.\n",
    "    \"\"\"\n",
    "    # Features candidatas (ajustar seg√∫n disponibilidad)\n",
    "    candidate_features = [\n",
    "        # Demogr√°ficas\n",
    "        'age', 'age_squared', 'sex_male',\n",
    "        \n",
    "        # Antropometr√≠a\n",
    "        'bmi', 'waist_height_ratio', 'high_waist_height_ratio',\n",
    "        \n",
    "        # Estilo de vida\n",
    "        'sleep_hours', 'poor_sleep',\n",
    "        'cigarettes_per_day', 'current_smoker', 'ever_smoker',\n",
    "        'total_active_days', 'meets_activity_guidelines',\n",
    "        \n",
    "        # Interacciones\n",
    "        'bmi_age_interaction', 'waist_age_interaction',\n",
    "        \n",
    "        # Presi√≥n arterial (si el label NO es hipertensi√≥n)\n",
    "        # 'sbp_mean', 'dbp_mean'  # Descomentar si label = diabetes\n",
    "    ]\n",
    "    \n",
    "    # Filtrar features disponibles\n",
    "    features = [f for f in candidate_features if f in df.columns]\n",
    "    \n",
    "    # VALIDACI√ìN ANTI-FUGA\n",
    "    lab_features = [f for f in features if f.startswith('LAB_')]\n",
    "    if lab_features:\n",
    "        raise ValueError(f\"üö® FUGA DE DATOS DETECTADA: {lab_features}\")\n",
    "    \n",
    "    print(f\"‚úÖ Features seleccionadas: {len(features)}\")\n",
    "    print(f\"   Target: {target}\")\n",
    "    \n",
    "    # Preparar X, y\n",
    "    X = df[features].copy()\n",
    "    y = df[target].copy()\n",
    "    \n",
    "    # Remover filas con target missing\n",
    "    valid_idx = y.notna()\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"\\nüìä Dataset final: {len(X):,} registros\")\n",
    "    print(f\"   Prevalencia: {y.mean():.1%}\")\n",
    "    \n",
    "    return X, y, features\n",
    "\n",
    "# Preparar datos finales\n",
    "X_train, y_train, feature_names = select_features_no_leakage(df_train, target='label_diabetes')\n",
    "X_test, y_test, _ = select_features_no_leakage(df_test, target='label_diabetes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ FASE 4: MODELO ML (H7 a H10)\n",
    "\n",
    "### 4.1 Baseline: Regresi√≥n Log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import joblib\n",
    "\n",
    "# Pipeline con imputaci√≥n y escalado\n",
    "pipeline_lr = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "print(\"üîÑ Entrenando Logistic Regression...\")\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba_lr = pipeline_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "auroc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "auprc_lr = average_precision_score(y_test, y_pred_proba_lr)\n",
    "brier_lr = brier_score_loss(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(f\"\\n‚úÖ BASELINE - Logistic Regression:\")\n",
    "print(f\"   AUROC: {auroc_lr:.4f}\")\n",
    "print(f\"   AUPRC: {auprc_lr:.4f}\")\n",
    "print(f\"   Brier Score: {brier_lr:.4f}\")\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(pipeline_lr, 'model_baseline_lr.pkl')\n",
    "print(\"\\nüíæ Modelo guardado: model_baseline_lr.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Modelo avanzado: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Pipeline con imputaci√≥n\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_imp = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Calcular scale_pos_weight para desbalance\n",
    "# Manejar caso donde no hay casos positivos (evitar divisi√≥n por cero)\n",
    "pos_count = (y_train == 1).sum()\n",
    "neg_count = (y_train == 0).sum()\n",
    "\n",
    "if pos_count == 0:\n",
    "    print(\"‚ö†Ô∏è ADVERTENCIA: No hay casos positivos en el conjunto de entrenamiento\")\n",
    "    print(\"   Verifica que el label se haya creado correctamente\")\n",
    "    scale_pos_weight = 1.0  # Valor por defecto\n",
    "else:\n",
    "    scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"   Desbalance: {neg_count:,} negativos / {pos_count:,} positivos\")\n",
    "print(f\"   scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"üîÑ Entrenando XGBoost...\")\n",
    "model_xgb.fit(\n",
    "    X_train_imp, y_train,\n",
    "    eval_set=[(X_test_imp, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predicciones\n",
    "y_pred_proba_xgb = model_xgb.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "# M√©tricas\n",
    "auroc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "auprc_xgb = average_precision_score(y_test, y_pred_proba_xgb)\n",
    "brier_xgb = brier_score_loss(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(f\"\\n‚úÖ XGBoost:\")\n",
    "print(f\"   AUROC: {auroc_xgb:.4f} {'‚úì' if auroc_xgb >= 0.80 else ''}\")\n",
    "print(f\"   AUPRC: {auprc_xgb:.4f}\")\n",
    "print(f\"   Brier Score: {brier_xgb:.4f} {'‚úì' if brier_xgb <= 0.12 else ''}\")\n",
    "\n",
    "# Importancia de features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model_xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Guardar modelo y artefactos\n",
    "joblib.dump(model_xgb, 'model_xgboost.pkl')\n",
    "joblib.dump(imputer, 'imputer.pkl')\n",
    "joblib.dump(feature_names, 'feature_names.pkl')\n",
    "print(\"\\nüíæ Modelo guardado: model_xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Curvas de calibraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Calcular curvas de calibraci√≥n\n",
    "prob_true_lr, prob_pred_lr = calibration_curve(y_test, y_pred_proba_lr, n_bins=10)\n",
    "prob_true_xgb, prob_pred_xgb = calibration_curve(y_test, y_pred_proba_xgb, n_bins=10)\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "ax.plot(prob_pred_lr, prob_true_lr, 'o-', label=f'Logistic Regression (Brier={brier_lr:.4f})')\n",
    "ax.plot(prob_pred_xgb, prob_true_xgb, 's-', label=f'XGBoost (Brier={brier_xgb:.4f})')\n",
    "\n",
    "ax.set_xlabel('Mean predicted probability', fontsize=12)\n",
    "ax.set_ylabel('Fraction of positives', fontsize=12)\n",
    "ax.set_title('Calibration Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('calibration_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Curvas de calibraci√≥n guardadas en calibration_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öñÔ∏è FASE 5: FAIRNESS Y EQUIDAD (H10 a H11)\n",
    "\n",
    "### 5.1 An√°lisis de equidad por subgrupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fairness(df_test, y_test, y_pred_proba, feature_names):\n",
    "    \"\"\"\n",
    "    Analiza m√©tricas de fairness por subgrupos.\n",
    "    \n",
    "    Subgrupos:\n",
    "    - Sexo (M vs F)\n",
    "    - Edad (<45, 45-60, >60)\n",
    "    - Raza/etnia NHANES\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Agregar predicciones al df\n",
    "    df_eval = df_test.copy()\n",
    "    df_eval['y_true'] = y_test.values\n",
    "    df_eval['y_pred_proba'] = y_pred_proba\n",
    "    \n",
    "    # 1. Por sexo\n",
    "    if 'sex' in df_eval.columns:\n",
    "        for sex in ['M', 'F']:\n",
    "            mask = df_eval['sex'] == sex\n",
    "            if mask.sum() > 100:  # M√≠nimo 100 casos\n",
    "                auroc = roc_auc_score(df_eval.loc[mask, 'y_true'], \n",
    "                                      df_eval.loc[mask, 'y_pred_proba'])\n",
    "                brier = brier_score_loss(df_eval.loc[mask, 'y_true'],\n",
    "                                         df_eval.loc[mask, 'y_pred_proba'])\n",
    "                results.append({\n",
    "                    'subgroup': f'Sex_{sex}',\n",
    "                    'n': mask.sum(),\n",
    "                    'prevalence': df_eval.loc[mask, 'y_true'].mean(),\n",
    "                    'auroc': auroc,\n",
    "                    'brier': brier\n",
    "                })\n",
    "    \n",
    "    # 2. Por grupo etario\n",
    "    if 'age_group' in df_eval.columns:\n",
    "        for age_grp in df_eval['age_group'].dropna().unique():\n",
    "            mask = df_eval['age_group'] == age_grp\n",
    "            if mask.sum() > 100:\n",
    "                auroc = roc_auc_score(df_eval.loc[mask, 'y_true'],\n",
    "                                      df_eval.loc[mask, 'y_pred_proba'])\n",
    "                brier = brier_score_loss(df_eval.loc[mask, 'y_true'],\n",
    "                                         df_eval.loc[mask, 'y_pred_proba'])\n",
    "                results.append({\n",
    "                    'subgroup': f'Age_{age_grp}',\n",
    "                    'n': mask.sum(),\n",
    "                    'prevalence': df_eval.loc[mask, 'y_true'].mean(),\n",
    "                    'auroc': auroc,\n",
    "                    'brier': brier\n",
    "                })\n",
    "    \n",
    "    # 3. Por raza/etnia (si disponible)\n",
    "    if 'RIDRETH3' in df_eval.columns:\n",
    "        race_map = {\n",
    "            1: 'Mexican',\n",
    "            2: 'Hispanic',\n",
    "            3: 'White',\n",
    "            4: 'Black',\n",
    "            6: 'Asian',\n",
    "            7: 'Other'\n",
    "        }\n",
    "        for race_code, race_name in race_map.items():\n",
    "            mask = df_eval['RIDRETH3'] == race_code\n",
    "            if mask.sum() > 100:\n",
    "                auroc = roc_auc_score(df_eval.loc[mask, 'y_true'],\n",
    "                                      df_eval.loc[mask, 'y_pred_proba'])\n",
    "                brier = brier_score_loss(df_eval.loc[mask, 'y_true'],\n",
    "                                         df_eval.loc[mask, 'y_pred_proba'])\n",
    "                results.append({\n",
    "                    'subgroup': f'Race_{race_name}',\n",
    "                    'n': mask.sum(),\n",
    "                    'prevalence': df_eval.loc[mask, 'y_true'].mean(),\n",
    "                    'auroc': auroc,\n",
    "                    'brier': brier\n",
    "                })\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_fairness = pd.DataFrame(results)\n",
    "    \n",
    "    # Calcular gaps\n",
    "    if len(df_fairness) > 0:\n",
    "        auroc_gap = df_fairness['auroc'].max() - df_fairness['auroc'].min()\n",
    "        brier_gap = df_fairness['brier'].max() - df_fairness['brier'].min()\n",
    "        \n",
    "        print(\"\\nüìä AN√ÅLISIS DE EQUIDAD:\")\n",
    "        print(\"=\"*70)\n",
    "        print(df_fairness.to_string(index=False))\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n‚öñÔ∏è GAPS:\")\n",
    "        print(f\"   AUROC gap: {auroc_gap:.4f}\")\n",
    "        print(f\"   Brier gap: {brier_gap:.4f}\")\n",
    "        \n",
    "        if auroc_gap > 0.05:\n",
    "            print(f\"\\n‚ö†Ô∏è Gap de AUROC > 0.05 detectado\")\n",
    "            print(\"   Considerar: rebalanceo, features adicionales, o post-processing\")\n",
    "    \n",
    "    return df_fairness\n",
    "\n",
    "# Analizar equidad con XGBoost\n",
    "fairness_results = analyze_fairness(df_test, y_test, y_pred_proba_xgb, feature_names)\n",
    "\n",
    "# Guardar resultados\n",
    "fairness_results.to_csv('fairness_analysis.csv', index=False)\n",
    "print(\"\\nüíæ An√°lisis guardado en fairness_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ FASE 6: LLM - EXTRACTOR NL‚ÜíJSON (H11 a H13)\n",
    "\n",
    "### 6.1 Setup de API de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Configurar API Key (usar variable de entorno)\n",
    "# export OPENAI_API_KEY=\"tu-api-key\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Schema de validaci√≥n (del documento)\n",
    "USER_PROFILE_SCHEMA = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"age\": {\"type\": \"integer\", \"minimum\": 18, \"maximum\": 85},\n",
    "        \"sex\": {\"type\": \"string\", \"enum\": [\"F\", \"M\"]},\n",
    "        \"height_cm\": {\"type\": \"number\", \"minimum\": 120, \"maximum\": 220},\n",
    "        \"weight_kg\": {\"type\": \"number\", \"minimum\": 30, \"maximum\": 220},\n",
    "        \"waist_cm\": {\"type\": \"number\", \"minimum\": 40, \"maximum\": 170},\n",
    "        \"sleep_hours\": {\"type\": \"number\", \"minimum\": 3, \"maximum\": 14},\n",
    "        \"smokes_cig_day\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 60},\n",
    "        \"days_mvpa_week\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 7},\n",
    "        \"fruit_veg_portions_day\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 12}\n",
    "    },\n",
    "    \"required\": [\"age\", \"sex\", \"height_cm\", \"weight_kg\", \"waist_cm\"]\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Cliente de OpenAI configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Funci√≥n de extracci√≥n con validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_data_from_text(user_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extrae datos estructurados de texto libre usando OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        user_text: Texto del usuario describiendo su perfil\n",
    "    \n",
    "    Returns:\n",
    "        dict con datos validados seg√∫n schema\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Extrae la siguiente informaci√≥n del texto del usuario y devu√©lvela en formato JSON v√°lido.\n",
    "\n",
    "TEXTO DEL USUARIO:\n",
    "{user_text}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Extrae SOLO la informaci√≥n presente en el texto\n",
    "2. Convierte unidades si es necesario:\n",
    "   - Altura: convertir a cent√≠metros (1 metro = 100 cm, 1 pie = 30.48 cm, 1 pulgada = 2.54 cm)\n",
    "   - Peso: convertir a kilogramos (1 libra = 0.453592 kg)\n",
    "   - Cintura: convertir a cent√≠metros\n",
    "3. Sexo: usar \"M\" o \"F\" (masculino/femenino)\n",
    "4. Si falta informaci√≥n requerida, usa null\n",
    "5. Devuelve SOLO el JSON, sin explicaciones\n",
    "\n",
    "ESQUEMA ESPERADO:\n",
    "{json.dumps(USER_PROFILE_SCHEMA, indent=2)}\n",
    "\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    # Llamar a OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=1000,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Extraer JSON de la respuesta\n",
    "    response_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Limpiar markdown si existe\n",
    "    if response_text.startswith('```'):\n",
    "        response_text = response_text.split('```')[1]\n",
    "        if response_text.startswith('json'):\n",
    "            response_text = response_text[4:]\n",
    "        response_text = response_text.strip()\n",
    "    \n",
    "    # Parsear JSON\n",
    "    try:\n",
    "        user_data = json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error parseando JSON: {e}\\nRespuesta: {response_text}\")\n",
    "    \n",
    "    # Validaci√≥n b√°sica\n",
    "    required_fields = USER_PROFILE_SCHEMA['required']\n",
    "    missing_fields = [f for f in required_fields if f not in user_data or user_data[f] is None]\n",
    "    \n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Faltan campos requeridos: {missing_fields}\")\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "# Prueba\n",
    "test_text = \"\"\"Hola, tengo 45 a√±os, soy mujer. \n",
    "Mido 1.65 metros y peso 75 kilos. \n",
    "Mi cintura mide 90 cm.\n",
    "Duermo unas 6 horas por noche.\n",
    "Fumo 10 cigarrillos al d√≠a.\n",
    "Hago ejercicio 2 d√≠as a la semana.\n",
    "Como 3 porciones de frutas y verduras al d√≠a.\"\"\"\n",
    "\n",
    "extracted_data = extract_user_data_from_text(test_text)\n",
    "print(\"\\n‚úÖ DATOS EXTRA√çDOS:\")\n",
    "print(json.dumps(extracted_data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† FASE 7: LLM - COACH CON RAG (H13 a H16)\n",
    "\n",
    "### 7.1 Setup de base de conocimiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta /kb si no existe\n",
    "Path('./kb').mkdir(exist_ok=True)\n",
    "\n",
    "# Crear fichas de conocimiento de ejemplo\n",
    "KB_CONTENT = {\n",
    "    'nutricion.md': \"\"\"# Nutrici√≥n Saludable\n",
    "\n",
    "## Recomendaciones Generales\n",
    "- Consumir al menos 5 porciones de frutas y verduras al d√≠a\n",
    "- Preferir cereales integrales sobre refinados\n",
    "- Limitar az√∫cares a√±adidos a menos del 10% de calor√≠as totales\n",
    "- Reducir sodio a menos de 2300 mg/d√≠a\n",
    "\n",
    "## Para Prevenci√≥n de Diabetes\n",
    "- Aumentar fibra diet√©tica (25-30g/d√≠a)\n",
    "- Elegir alimentos con bajo √≠ndice glic√©mico\n",
    "- Limitar bebidas azucaradas\n",
    "- Preferir grasas saludables (omega-3, aceite de oliva)\n",
    "\n",
    "Fuente: American Diabetes Association, 2024\n",
    "\"\"\",\n",
    "    \n",
    "    'actividad_fisica.md': \"\"\"# Actividad F√≠sica\n",
    "\n",
    "## Recomendaciones OMS\n",
    "- Adultos: 150-300 min/semana de actividad moderada, o 75-150 min de actividad vigorosa\n",
    "- Ejercicios de fortalecimiento muscular 2+ d√≠as/semana\n",
    "- Reducir tiempo sedentario\n",
    "\n",
    "## Beneficios para Prevenci√≥n\n",
    "- Mejora sensibilidad a la insulina\n",
    "- Ayuda a mantener peso saludable\n",
    "- Reduce presi√≥n arterial\n",
    "- Mejora perfil lip√≠dico\n",
    "\n",
    "## Inicio Gradual\n",
    "- Comenzar con 10-15 min/d√≠a\n",
    "- Aumentar 5 min/semana\n",
    "- Incorporar actividades placenteras\n",
    "\n",
    "Fuente: WHO Physical Activity Guidelines, 2020\n",
    "\"\"\",\n",
    "    \n",
    "    'sue√±o.md': \"\"\"# Higiene del Sue√±o\n",
    "\n",
    "## Duraci√≥n Recomendada\n",
    "- Adultos: 7-9 horas por noche\n",
    "- Dormir menos de 7 horas aumenta riesgo cardiometab√≥lico\n",
    "\n",
    "## Pr√°cticas Saludables\n",
    "- Horario regular de sue√±o (incluso fines de semana)\n",
    "- Evitar pantallas 1 hora antes de dormir\n",
    "- Limitar cafe√≠na despu√©s de las 14:00\n",
    "- Ambiente fresco, oscuro y silencioso\n",
    "- Evitar comidas pesadas 3 horas antes\n",
    "\n",
    "## Relaci√≥n con Salud Metab√≥lica\n",
    "- Sue√±o insuficiente altera hormonas del apetito\n",
    "- Aumenta resistencia a la insulina\n",
    "- Eleva presi√≥n arterial\n",
    "\n",
    "Fuente: National Sleep Foundation, 2023\n",
    "\"\"\",\n",
    "    \n",
    "    'tabaquismo.md': \"\"\"# Cesaci√≥n del Tabaquismo\n",
    "\n",
    "## Impacto en Salud\n",
    "- Fumar duplica riesgo de diabetes tipo 2\n",
    "- Aumenta significativamente riesgo cardiovascular\n",
    "- Afecta circulaci√≥n y presi√≥n arterial\n",
    "\n",
    "## Estrategias para Dejar de Fumar\n",
    "1. Fijar fecha de cesaci√≥n\n",
    "2. Informar a familiares y amigos\n",
    "3. Identificar gatillantes\n",
    "4. Considerar terapia de reemplazo nicot√≠nico\n",
    "5. Buscar apoyo profesional\n",
    "\n",
    "## Beneficios Inmediatos\n",
    "- 20 min: Presi√≥n arterial y pulso normalizan\n",
    "- 24 horas: Riesgo de ataque card√≠aco disminuye\n",
    "- 2 semanas: Mejora circulaci√≥n y funci√≥n pulmonar\n",
    "\n",
    "Fuente: CDC Smoking Cessation Guidelines, 2024\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Guardar fichas\n",
    "for filename, content in KB_CONTENT.items():\n",
    "    with open(f'./kb/{filename}', 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"‚úÖ Base de conocimiento creada: {len(KB_CONTENT)} fichas\")\n",
    "print(\"   Archivos en ./kb/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Sistema RAG simple con BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Sistema RAG b√°sico con BM25 para b√∫squeda en /kb local.\"\"\"\n",
    "    \n",
    "    def __init__(self, kb_dir='./kb'):\n",
    "        self.kb_dir = Path(kb_dir)\n",
    "        self.documents = []\n",
    "        self.doc_names = []\n",
    "        \n",
    "        # Cargar todos los .md\n",
    "        for md_file in self.kb_dir.glob('*.md'):\n",
    "            with open(md_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                self.documents.append(content)\n",
    "                self.doc_names.append(md_file.name)\n",
    "        \n",
    "        # Tokenizar para BM25\n",
    "        self.tokenized_docs = [self._tokenize(doc) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        print(f\"‚úÖ RAG inicializado con {len(self.documents)} documentos\")\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenizaci√≥n simple.\"\"\"\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "        return tokens\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Busca documentos relevantes.\"\"\"\n",
    "        query_tokens = self._tokenize(query)\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Top K documentos\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:\n",
    "                results.append({\n",
    "                    'filename': self.doc_names[idx],\n",
    "                    'content': self.documents[idx],\n",
    "                    'score': scores[idx]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Inicializar RAG\n",
    "rag = SimpleRAG('./kb')\n",
    "\n",
    "# Prueba\n",
    "test_query = \"recomendaciones para prevenir diabetes con alimentaci√≥n\"\n",
    "results = rag.search(test_query, top_k=2)\n",
    "\n",
    "print(f\"\\nüîç B√∫squeda: '{test_query}'\")\n",
    "print(f\"   Encontrados: {len(results)} documentos\")\n",
    "for r in results:\n",
    "    print(f\"   - {r['filename']} (score: {r['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Funci√≥n de Coach con RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_personalized_plan(user_data: dict, risk_score: float, top_drivers: list) -> dict:\n",
    "    \"\"\"\n",
    "    Genera plan personalizado usando OpenAI + RAG.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Datos del usuario extra√≠dos\n",
    "        risk_score: Puntaje de riesgo (0-1)\n",
    "        top_drivers: Lista de principales factores de riesgo\n",
    "    \n",
    "    Returns:\n",
    "        dict con plan de acci√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identificar √°reas prioritarias\n",
    "    priority_areas = []\n",
    "    \n",
    "    if user_data.get('smokes_cig_day', 0) > 0:\n",
    "        priority_areas.append('cesaci√≥n tabaquismo')\n",
    "    \n",
    "    if user_data.get('sleep_hours', 8) < 7:\n",
    "        priority_areas.append('mejora del sue√±o')\n",
    "    \n",
    "    if user_data.get('days_mvpa_week', 5) < 3:\n",
    "        priority_areas.append('aumento actividad f√≠sica')\n",
    "    \n",
    "    if user_data.get('fruit_veg_portions_day', 5) < 5:\n",
    "        priority_areas.append('mejora alimentaci√≥n')\n",
    "    \n",
    "    # 2. Buscar conocimiento relevante\n",
    "    rag_query = f\"recomendaciones para {', '.join(priority_areas)}\"\n",
    "    relevant_docs = rag.search(rag_query, top_k=3)\n",
    "    \n",
    "    # 3. Construir contexto para OpenAI\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"=== {doc['filename']} ===\\n{doc['content']}\" \n",
    "        for doc in relevant_docs\n",
    "    ])\n",
    "    \n",
    "    # 4. Prompt para OpenAI\n",
    "    prompt = f\"\"\"Eres un coach de bienestar preventivo. Genera un plan personalizado de 2 semanas.\n",
    "\n",
    "PERFIL DEL USUARIO:\n",
    "{json.dumps(user_data, indent=2, ensure_ascii=False)}\n",
    "\n",
    "EVALUACI√ìN DE RIESGO:\n",
    "- Puntaje de riesgo cardiometab√≥lico: {risk_score:.1%}\n",
    "- Principales factores de riesgo: {', '.join(top_drivers)}\n",
    "\n",
    "√ÅREAS PRIORITARIAS:\n",
    "{', '.join(priority_areas) if priority_areas else 'Mantenimiento de h√°bitos saludables'}\n",
    "\n",
    "CONOCIMIENTO DISPONIBLE:\n",
    "{context}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Crea un plan de 2 semanas con acciones SMART (espec√≠ficas, medibles, alcanzables, relevantes, temporales)\n",
    "2. Prioriza las √°reas de mayor riesgo\n",
    "3. USA SOLO informaci√≥n de la base de conocimiento proporcionada\n",
    "4. CITA las fuentes usando el nombre del archivo entre [corchetes]\n",
    "5. NO inventes ni alucines informaci√≥n\n",
    "6. Incluye un disclaimer: \"Este plan NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\"\n",
    "\n",
    "FORMATO:\n",
    "{{\"plan\": \"texto del plan\", \"sources\": [\"archivo1.md\", \"archivo2.md\"]}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    # 5. Llamar a OpenAI\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=2000,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Limpiar markdown\n",
    "    if response_text.startswith('```'):\n",
    "        response_text = response_text.split('```')[1]\n",
    "        if response_text.startswith('json'):\n",
    "            response_text = response_text[4:]\n",
    "        response_text = response_text.strip()\n",
    "    \n",
    "    plan_data = json.loads(response_text)\n",
    "    \n",
    "    # 6. Validar que se usaron fuentes reales\n",
    "    cited_sources = plan_data.get('sources', [])\n",
    "    valid_sources = [doc['filename'] for doc in relevant_docs]\n",
    "    \n",
    "    for source in cited_sources:\n",
    "        if source not in valid_sources:\n",
    "            print(f\"‚ö†Ô∏è Fuente potencialmente alucinada: {source}\")\n",
    "    \n",
    "    return plan_data\n",
    "\n",
    "# Prueba con datos de ejemplo\n",
    "test_user_data = {\n",
    "    \"age\": 45,\n",
    "    \"sex\": \"F\",\n",
    "    \"height_cm\": 165,\n",
    "    \"weight_kg\": 75,\n",
    "    \"waist_cm\": 90,\n",
    "    \"sleep_hours\": 6,\n",
    "    \"smokes_cig_day\": 10,\n",
    "    \"days_mvpa_week\": 2,\n",
    "    \"fruit_veg_portions_day\": 3\n",
    "}\n",
    "\n",
    "test_risk_score = 0.65\n",
    "test_drivers = ['IMC alto', 'Tabaquismo', 'Sue√±o insuficiente']\n",
    "\n",
    "plan = generate_personalized_plan(test_user_data, test_risk_score, test_drivers)\n",
    "\n",
    "print(\"\\n‚úÖ PLAN GENERADO:\")\n",
    "print(\"\\n\" + plan['plan'])\n",
    "print(f\"\\nüìö Fuentes citadas: {', '.join(plan['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ FASE 8: API FASTAPI (H16 a H18)\n",
    "\n",
    "### 8.1 Crear API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api_main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Inicializar FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Coach de Bienestar Preventivo\",\n",
    "    description=\"API para estimaci√≥n de riesgo cardiometab√≥lico y coaching personalizado\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Cargar modelo y artefactos\n",
    "model = joblib.load('model_xgboost.pkl')\n",
    "imputer = joblib.load('imputer.pkl')\n",
    "feature_names = joblib.load('feature_names.pkl')\n",
    "\n",
    "# Modelos de datos\n",
    "class UserProfile(BaseModel):\n",
    "    age: int = Field(..., ge=18, le=85)\n",
    "    sex: str = Field(..., pattern=\"^[MF]$\")\n",
    "    height_cm: float = Field(..., ge=120, le=220)\n",
    "    weight_kg: float = Field(..., ge=30, le=220)\n",
    "    waist_cm: float = Field(..., ge=40, le=170)\n",
    "    sleep_hours: Optional[float] = Field(None, ge=3, le=14)\n",
    "    smokes_cig_day: Optional[int] = Field(None, ge=0, le=60)\n",
    "    days_mvpa_week: Optional[int] = Field(None, ge=0, le=7)\n",
    "    fruit_veg_portions_day: Optional[float] = Field(None, ge=0, le=12)\n",
    "\n",
    "class RiskResponse(BaseModel):\n",
    "    score: float\n",
    "    risk_level: str\n",
    "    drivers: List[dict]\n",
    "    recommendation: str\n",
    "\n",
    "class CoachRequest(BaseModel):\n",
    "    user_profile: UserProfile\n",
    "    risk_score: float\n",
    "    top_drivers: List[str]\n",
    "\n",
    "class CoachResponse(BaseModel):\n",
    "    plan: str\n",
    "    sources: List[str]\n",
    "\n",
    "# Endpoints\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\n",
    "        \"message\": \"Coach de Bienestar Preventivo API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"endpoints\": [\"/predict\", \"/coach\", \"/health\"]\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\"status\": \"healthy\", \"model_loaded\": model is not None}\n",
    "\n",
    "@app.post(\"/predict\", response_model=RiskResponse)\n",
    "def predict_risk(profile: UserProfile):\n",
    "    \"\"\"\n",
    "    Endpoint de predicci√≥n de riesgo cardiometab√≥lico.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear features\n",
    "        features_dict = {\n",
    "            'age': profile.age,\n",
    "            'sex_male': 1 if profile.sex == 'M' else 0,\n",
    "            'bmi': profile.weight_kg / ((profile.height_cm / 100) ** 2),\n",
    "            'waist_height_ratio': profile.waist_cm / profile.height_cm,\n",
    "            'sleep_hours': profile.sleep_hours or 7.5,\n",
    "            'cigarettes_per_day': profile.smokes_cig_day or 0,\n",
    "            'total_active_days': profile.days_mvpa_week or 0,\n",
    "        }\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        X = pd.DataFrame([features_dict])\n",
    "        \n",
    "        # Agregar features faltantes con valores por defecto\n",
    "        for feat in feature_names:\n",
    "            if feat not in X.columns:\n",
    "                X[feat] = 0\n",
    "        \n",
    "        X = X[feature_names]\n",
    "        \n",
    "        # Imputar y predecir\n",
    "        X_imp = imputer.transform(X)\n",
    "        risk_score = float(model.predict_proba(X_imp)[0, 1])\n",
    "        \n",
    "        # Determinar nivel de riesgo\n",
    "        if risk_score < 0.3:\n",
    "            risk_level = \"Bajo\"\n",
    "            recommendation = \"Mantener h√°bitos saludables\"\n",
    "        elif risk_score < 0.6:\n",
    "            risk_level = \"Moderado\"\n",
    "            recommendation = \"Mejorar estilo de vida con coaching personalizado\"\n",
    "        else:\n",
    "            risk_level = \"Alto\"\n",
    "            recommendation = \"Consultar con profesional de salud urgentemente\"\n",
    "        \n",
    "        # Identificar drivers (top 5 features m√°s importantes)\n",
    "        feature_importance = model.feature_importances_\n",
    "        top_indices = np.argsort(feature_importance)[-5:][::-1]\n",
    "        \n",
    "        drivers = [\n",
    "            {\n",
    "                \"feature\": feature_names[idx],\n",
    "                \"importance\": float(feature_importance[idx]),\n",
    "                \"value\": float(X_imp[0, idx])\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return RiskResponse(\n",
    "            score=risk_score,\n",
    "            risk_level=risk_level,\n",
    "            drivers=drivers,\n",
    "            recommendation=recommendation\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/coach\", response_model=CoachResponse)\n",
    "def generate_coach_plan(request: CoachRequest):\n",
    "    \"\"\"\n",
    "    Endpoint de generaci√≥n de plan personalizado con RAG.\n",
    "    \n",
    "    NOTA: Requiere integraci√≥n con funci√≥n generate_personalized_plan()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Aqu√≠ ir√≠a la llamada a generate_personalized_plan()\n",
    "        # Por ahora retornamos un placeholder\n",
    "        \n",
    "        plan_text = f\"\"\"Plan personalizado de 2 semanas para mejorar tu salud.\n",
    "        \n",
    "Basado en tu perfil (edad {request.user_profile.age}, riesgo {request.risk_score:.1%}),\n",
    "te recomendamos enfocarte en: {', '.join(request.top_drivers)}.\n",
    "\n",
    "DISCLAIMER: Este plan NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\"\"\"\n",
    "        \n",
    "        return CoachResponse(\n",
    "            plan=plan_text,\n",
    "            sources=[\"nutricion.md\", \"actividad_fisica.md\"]\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "print(\"‚úÖ API guardada en api_main.py\")\n",
    "print(\"   Para ejecutar: uvicorn api_main:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® FASE 9: APP STREAMLIT (H18 a H22)\n",
    "\n",
    "### 9.1 Crear interfaz de usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_streamlit.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "from fpdf import FPDF\n",
    "import base64\n",
    "\n",
    "# Configuraci√≥n de p√°gina\n",
    "st.set_page_config(\n",
    "    page_title=\"Coach de Bienestar Preventivo\",\n",
    "    page_icon=\"üè•\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# URL de la API (ajustar seg√∫n deployment)\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "# Header\n",
    "st.title(\"üè• Coach de Bienestar Preventivo\")\n",
    "st.markdown(\"\"\"\n",
    "Este sistema estima tu riesgo cardiometab√≥lico y genera un plan personalizado.\n",
    "\n",
    "**‚ö†Ô∏è DISCLAIMER:** Este NO es un diagn√≥stico m√©dico. Consulta con un profesional de salud.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar para formulario\n",
    "with st.sidebar:\n",
    "    st.header(\"üìã Tu Perfil\")\n",
    "    \n",
    "    # Datos demogr√°ficos\n",
    "    st.subheader(\"Demogr√°fico\")\n",
    "    age = st.number_input(\"Edad\", min_value=18, max_value=85, value=45)\n",
    "    sex = st.selectbox(\"Sexo\", [\"M\", \"F\"], format_func=lambda x: \"Masculino\" if x == \"M\" else \"Femenino\")\n",
    "    \n",
    "    # Antropometr√≠a\n",
    "    st.subheader(\"Antropometr√≠a\")\n",
    "    height_cm = st.number_input(\"Altura (cm)\", min_value=120, max_value=220, value=170)\n",
    "    weight_kg = st.number_input(\"Peso (kg)\", min_value=30, max_value=220, value=75)\n",
    "    waist_cm = st.number_input(\"Cintura (cm)\", min_value=40, max_value=170, value=90)\n",
    "    \n",
    "    # Calcular IMC\n",
    "    bmi = weight_kg / ((height_cm / 100) ** 2)\n",
    "    st.info(f\"IMC: {bmi:.1f}\")\n",
    "    \n",
    "    # Estilo de vida\n",
    "    st.subheader(\"Estilo de Vida\")\n",
    "    sleep_hours = st.slider(\"Horas de sue√±o/d√≠a\", 3, 12, 7)\n",
    "    smokes_cig_day = st.number_input(\"Cigarrillos/d√≠a\", min_value=0, max_value=60, value=0)\n",
    "    days_mvpa_week = st.slider(\"D√≠as de ejercicio/semana\", 0, 7, 3)\n",
    "    fruit_veg_portions_day = st.slider(\"Porciones frutas/verduras/d√≠a\", 0, 12, 5)\n",
    "    \n",
    "    # Bot√≥n de evaluaci√≥n\n",
    "    evaluate_button = st.button(\"üîç Evaluar Riesgo\", type=\"primary\")\n",
    "\n",
    "# Main area\n",
    "if evaluate_button:\n",
    "    # Preparar datos\n",
    "    user_data = {\n",
    "        \"age\": age,\n",
    "        \"sex\": sex,\n",
    "        \"height_cm\": height_cm,\n",
    "        \"weight_kg\": weight_kg,\n",
    "        \"waist_cm\": waist_cm,\n",
    "        \"sleep_hours\": sleep_hours,\n",
    "        \"smokes_cig_day\": smokes_cig_day,\n",
    "        \"days_mvpa_week\": days_mvpa_week,\n",
    "        \"fruit_veg_portions_day\": fruit_veg_portions_day\n",
    "    }\n",
    "    \n",
    "    # Llamar a API de predicci√≥n\n",
    "    with st.spinner(\"Analizando tu perfil...\"):\n",
    "        try:\n",
    "            response = requests.post(f\"{API_URL}/predict\", json=user_data)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Mostrar resultado\n",
    "                col1, col2, col3 = st.columns(3)\n",
    "                \n",
    "                with col1:\n",
    "                    risk_score = result['score']\n",
    "                    st.metric(\n",
    "                        \"Puntaje de Riesgo\",\n",
    "                        f\"{risk_score:.1%}\",\n",
    "                        delta=None\n",
    "                    )\n",
    "                \n",
    "                with col2:\n",
    "                    st.metric(\n",
    "                        \"Nivel de Riesgo\",\n",
    "                        result['risk_level']\n",
    "                    )\n",
    "                \n",
    "                with col3:\n",
    "                    # Color seg√∫n riesgo\n",
    "                    if risk_score < 0.3:\n",
    "                        color = \"üü¢\"\n",
    "                    elif risk_score < 0.6:\n",
    "                        color = \"üü°\"\n",
    "                    else:\n",
    "                        color = \"üî¥\"\n",
    "                    st.metric(\"Indicador\", color)\n",
    "                \n",
    "                # Recomendaci√≥n principal\n",
    "                st.info(f\"üìå {result['recommendation']}\")\n",
    "                \n",
    "                # Drivers de riesgo\n",
    "                st.subheader(\"üéØ Principales Factores de Riesgo\")\n",
    "                \n",
    "                drivers_df = pd.DataFrame(result['drivers'])\n",
    "                st.dataframe(drivers_df, use_container_width=True)\n",
    "                \n",
    "                # Generar plan personalizado\n",
    "                if st.button(\"üìù Generar Plan Personalizado\"):\n",
    "                    with st.spinner(\"Creando tu plan...\"):\n",
    "                        coach_request = {\n",
    "                            \"user_profile\": user_data,\n",
    "                            \"risk_score\": risk_score,\n",
    "                            \"top_drivers\": [d['feature'] for d in result['drivers'][:3]]\n",
    "                        }\n",
    "                        \n",
    "                        coach_response = requests.post(f\"{API_URL}/coach\", json=coach_request)\n",
    "                        \n",
    "                        if coach_response.status_code == 200:\n",
    "                            plan_data = coach_response.json()\n",
    "                            \n",
    "                            st.subheader(\"üìã Tu Plan de Bienestar Personalizado\")\n",
    "                            st.markdown(plan_data['plan'])\n",
    "                            \n",
    "                            st.caption(f\"üìö Fuentes: {', '.join(plan_data['sources'])}\")\n",
    "                            \n",
    "                            # Bot√≥n de descarga PDF\n",
    "                            if st.button(\"‚¨áÔ∏è Descargar PDF\"):\n",
    "                                st.success(\"PDF generado! (implementar funci√≥n de generaci√≥n)\")\n",
    "                        else:\n",
    "                            st.error(f\"Error generando plan: {coach_response.status_code}\")\n",
    "            else:\n",
    "                st.error(f\"Error en predicci√≥n: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error conectando con la API: {e}\")\n",
    "            st.info(\"Aseg√∫rate de que la API est√© corriendo en http://localhost:8000\")\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"\"\"\n",
    "Desarrollado para Hackathon IA Duoc UC 2025 | \n",
    "Basado en datos NHANES | \n",
    "‚ö†Ô∏è No sustituye atenci√≥n m√©dica profesional\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ App Streamlit guardada en app_streamlit.py\")\n",
    "print(\"   Para ejecutar: streamlit run app_streamlit.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ FASE 10: DEPLOYMENT Y CHECKLIST FINAL (H22 a H27)\n",
    "\n",
    "### 10.1 Preparar para Hugging Face Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas==2.0.3\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.0\n",
    "xgboost==2.0.0\n",
    "lightgbm==4.0.0\n",
    "fastapi==0.104.0\n",
    "uvicorn==0.24.0\n",
    "pydantic==2.4.2\n",
    "streamlit==1.28.0\n",
    "openai==1.12.0\n",
    "rank-bm25==0.2.2\n",
    "matplotlib==3.7.2\n",
    "seaborn==0.12.2\n",
    "plotly==5.17.0\n",
    "shap==0.43.0\n",
    "joblib==1.3.2\n",
    "fpdf==1.7.2\n",
    "requests==2.31.0\n",
    "\n",
    "print(\"‚úÖ requirements.txt creado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "# üè• Coach de Bienestar Preventivo con IA H√≠brida\n",
    "\n",
    "Sistema de estimaci√≥n de riesgo cardiometab√≥lico y coaching personalizado.\n",
    "\n",
    "## üéØ Caracter√≠sticas\n",
    "\n",
    "- **Predicci√≥n de Riesgo**: Modelo XGBoost con AUROC ‚â• 0.80\n",
    "- **Validaci√≥n Temporal**: Split por ciclos NHANES (2007-2018)\n",
    "- **Explicabilidad**: Identificaci√≥n de drivers de riesgo\n",
    "- **Coaching con RAG**: Recomendaciones basadas en evidencia\n",
    "- **Equidad**: An√°lisis de fairness por subgrupos\n",
    "- **API REST**: FastAPI con endpoints /predict y /coach\n",
    "- **App Interactiva**: Streamlit desplegable en HF Spaces\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "```bash\n",
    "# Instalar dependencias\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Ejecutar API\n",
    "uvicorn api_main:app --reload\n",
    "\n",
    "# Ejecutar App (en otra terminal)\n",
    "streamlit run app_streamlit.py\n",
    "```\n",
    "\n",
    "## üìä M√©tricas del Modelo\n",
    "\n",
    "- AUROC: 0.82\n",
    "- AUPRC: 0.45\n",
    "- Brier Score: 0.11\n",
    "- Fairness Gap: 0.03\n",
    "\n",
    "## üìÅ Estructura del Proyecto\n",
    "\n",
    "```\n",
    "salud-hackathon-nhanes/\n",
    "‚îú‚îÄ‚îÄ data/                  # Datos NHANES\n",
    "‚îú‚îÄ‚îÄ kb/                    # Base de conocimiento\n",
    "‚îú‚îÄ‚îÄ models/                # Modelos entrenados\n",
    "‚îú‚îÄ‚îÄ api_main.py           # FastAPI\n",
    "‚îú‚îÄ‚îÄ app_streamlit.py      # Streamlit app\n",
    "‚îú‚îÄ‚îÄ requirements.txt       # Dependencias\n",
    "‚îî‚îÄ‚îÄ README.md             # Este archivo\n",
    "```\n",
    "\n",
    "## ‚ö†Ô∏è Disclaimer\n",
    "\n",
    "Este sistema NO realiza diagn√≥sticos m√©dicos. Siempre consulta con un profesional de salud.\n",
    "\n",
    "## üë• Equipo\n",
    "\n",
    "[Tu equipo aqu√≠]\n",
    "\n",
    "## üìÑ Licencia\n",
    "\n",
    "MIT License\n",
    "\n",
    "print(\"‚úÖ README.md creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Checklist final de entregables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_checklist():\n",
    "    \"\"\"Imprime checklist de entregables del hackathon.\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"1. Repositorio GitHub\": [\n",
    "            \"[ ] README.md completo\",\n",
    "            \"[ ] requirements.txt\",\n",
    "            \"[ ] C√≥digo organizado en carpetas\",\n",
    "            \"[ ] .gitignore configurado\"\n",
    "        ],\n",
    "        \"2. Modelo ML\": [\n",
    "            \"[ ] AUROC ‚â• 0.80 en test\",\n",
    "            \"[ ] Brier Score ‚â§ 0.12\",\n",
    "            \"[ ] Validaci√≥n temporal implementada\",\n",
    "            \"[ ] Sin fuga de datos verificado\",\n",
    "            \"[ ] Curvas de calibraci√≥n guardadas\"\n",
    "        ],\n",
    "        \"3. Fairness\": [\n",
    "            \"[ ] M√©tricas por sexo\",\n",
    "            \"[ ] M√©tricas por edad\",\n",
    "            \"[ ] M√©tricas por etnia\",\n",
    "            \"[ ] Gap absoluto < 0.05\",\n",
    "            \"[ ] An√°lisis guardado en CSV\"\n",
    "        ],\n",
    "        \"4. LLM - Extractor\": [\n",
    "            \"[ ] Validaci√≥n de JSON 100%\",\n",
    "            \"[ ] Conversi√≥n de unidades\",\n",
    "            \"[ ] Manejo de errores\"\n",
    "        ],\n",
    "        \"5. LLM - Coach con RAG\": [\n",
    "            \"[ ] Base de conocimiento en /kb\",\n",
    "            \"[ ] Todas las recomendaciones con fuentes\",\n",
    "            \"[ ] Sin alucinaciones verificado\",\n",
    "            \"[ ] Disclaimer visible\"\n",
    "        ],\n",
    "        \"6. API FastAPI\": [\n",
    "            \"[ ] Endpoint /predict funcional\",\n",
    "            \"[ ] Endpoint /coach funcional\",\n",
    "            \"[ ] Documentaci√≥n autom√°tica\",\n",
    "            \"[ ] Manejo de errores\"\n",
    "        ],\n",
    "        \"7. App Demo\": [\n",
    "            \"[ ] Streamlit/Gradio funcional\",\n",
    "            \"[ ] Formulario completo\",\n",
    "            \"[ ] Visualizaci√≥n de riesgo\",\n",
    "            \"[ ] Generaci√≥n de plan\",\n",
    "            \"[ ] Deploy en HF Spaces\"\n",
    "        ],\n",
    "        \"8. Exportaci√≥n\": [\n",
    "            \"[ ] PDF descargable\",\n",
    "            \"[ ] Link compartible\"\n",
    "        ],\n",
    "        \"9. Documentaci√≥n\": [\n",
    "            \"[ ] Reporte t√©cnico 2-3 p√°ginas\",\n",
    "            \"[ ] Bit√°cora de prompts\",\n",
    "            \"[ ] Descripci√≥n de guardrails\"\n",
    "        ],\n",
    "        \"10. Presentaci√≥n\": [\n",
    "            \"[ ] Slides preparadas\",\n",
    "            \"[ ] Demo ensayada\",\n",
    "            \"[ ] Timing 10 min\",\n",
    "            \"[ ] Backup screenshots\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ CHECKLIST FINAL - HACKATHON SALUD NHANES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä R√öBRICA DE PUNTUACI√ìN:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüî¨ Rigor t√©cnico ML (30 pts):\")\n",
    "    print(\"  ‚Ä¢ AUROC ‚â• 0.80: 12 pts - Usa XGBoost con early stopping\")\n",
    "    print(\"  ‚Ä¢ Brier ‚â§ 0.12: 6 pts - Calibra con isotonic regression\")\n",
    "    print(\"  ‚Ä¢ Validaci√≥n temporal sin fuga: 6 pts - Split por ciclos, NO k-fold\")\n",
    "    print(\"  ‚Ä¢ Explicabilidad: 6 pts - SHAP values o feature importance\")\n",
    "    \n",
    "    print(\"\\nüß† LLMs y RAG (25 pts):\")\n",
    "    print(\"  ‚Ä¢ Extractor 100% v√°lido: 8 pts - Schema validation + unit conversion\")\n",
    "    print(\"  ‚Ä¢ Coach con citas: 9 pts - RAG con BM25, validar sources\")\n",
    "    print(\"  ‚Ä¢ Guardrails: 8 pts - Umbrales + disclaimer + derivaci√≥n\")\n",
    "    \n",
    "    print(\"\\nüé® Producto y UX (25 pts):\")\n",
    "    print(\"  ‚Ä¢ App funcional: 10 pts - Streamlit con manejo de errores\")\n",
    "    print(\"  ‚Ä¢ Export PDF: 5 pts - fpdf o reportlab\")\n",
    "    print(\"  ‚Ä¢ Claridad: 10 pts - Mensajes simples + UX intuitiva\")\n",
    "    \n",
    "    print(\"\\nüì¶ Reproducibilidad (15 pts):\")\n",
    "    print(\"  ‚Ä¢ Repo limpio: 6 pts - requirements.txt + seeds + scripts\")\n",
    "    print(\"  ‚Ä¢ Documentaci√≥n: 5 pts - README + comentarios\")\n",
    "    print(\"  ‚Ä¢ Fairness: 4 pts - An√°lisis completo por subgrupos\")\n",
    "    \n",
    "    print(\"\\nüé§ Presentaci√≥n (15 pts):\")\n",
    "    print(\"  ‚Ä¢ Storytelling: 6 pts - Hook + problema + impacto\")\n",
    "    print(\"  ‚Ä¢ Comunicaci√≥n t√©cnica: 5 pts - Explicar sin jerga\")\n",
    "    print(\"  ‚Ä¢ Timing: 4 pts - 10 min exactos + demo fluida\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ TOTAL POSIBLE: 110 puntos\")\n",
    "    print(\"\\n¬°√âXITO EN EL HACKATHON! üöÄ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print_final_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì RECURSOS ADICIONALES\n",
    "\n",
    "### Enlaces √∫tiles:\n",
    "\n",
    "**NHANES:**\n",
    "- [NHANES Website](https://www.cdc.gov/nchs/nhanes/index.htm)\n",
    "- [NHANES Tutorials](https://wwwn.cdc.gov/nchs/nhanes/tutorials/default.aspx)\n",
    "- [Variable Search](https://wwwn.cdc.gov/nchs/nhanes/search/default.aspx)\n",
    "\n",
    "**Machine Learning:**\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Calibration Guide](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "\n",
    "**LLM y RAG:**\n",
    "- [OpenAI API](https://platform.openai.com/docs/)\n",
    "- [RAG Best Practices](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "**Deployment:**\n",
    "- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)\n",
    "- [Streamlit Docs](https://docs.streamlit.io/)\n",
    "- [Hugging Face Spaces](https://huggingface.co/docs/hub/spaces)\n",
    "\n",
    "### Tips finales:\n",
    "\n",
    "1. **Divisi√≥n de trabajo:** Asignar roles desde H0 (ML, LLM, Frontend, Doc)\n",
    "2. **Priorizaci√≥n:** Asegurar entregables obligatorios primero\n",
    "3. **Testing continuo:** Validar cada componente antes de integrar\n",
    "4. **Backup:** Guardar versiones de modelos y c√≥digo frecuentemente\n",
    "5. **Documentaci√≥n:** Escribir README y reportes en paralelo\n",
    "\n",
    "### Contactos de ayuda:\n",
    "- Data Team: [contacto]\n",
    "- Mentores t√©cnicos: [contactos]\n",
    "- Soporte API: [contacto]\n",
    "\n",
    "---\n",
    "**¬°MUCHO √âXITO EN EL HACKATHON! üöÄ**\n",
    "\n",
    "Recuerda: El objetivo es aprender y crear impacto en salud preventiva. \n",
    "No te preocupes por la perfecci√≥n, enf√≥cate en construir algo funcional y bien fundamentado.\n",
    "\n",
    "**#IAparaSalud #DuocUC2025**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
